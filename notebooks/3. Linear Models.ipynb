{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "<h2 style=\"font-size:34px; font-family:Verdana\" align=\"center\">Linear Models </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Classification is a fundamental issue in machine learning and data mining. In classification, the goal of a learning algorithm is to construct a classifier given a set of training examples with class labels. Typically, an example $E$ is represented by a tuple of attribute values ($x_1, x_2, … , x_n$), where $x_i$ is the value of attribute $X_i$. \n",
    "\n",
    "Let $C$ represent the classification variable, and let $c$ be the value of $C$. In this paper, we assume that there are only two classes: + (the positive class) or − (the negative class).\n",
    "\n",
    "A classifier is a function that assigns a class label to an example. From the probability perspective, according to Rule, the probability of an example $E = (x_1, x_2, … , x_n)$ being class $c$ is\n",
    "\n",
    "$$p(c|E) = {\\frac{p(E|c)p(c)}{p(E)}}$$\n",
    "\n",
    "$E$ is classified as the class $C = +$ if and only if\n",
    "$$f_b(E) = {\\frac{p(C = +|E)}{p(C = -|E)}}\\geq1$$\n",
    "\n",
    "\n",
    "where $f_b(E)$ is called a Bayesian classifier. Assume that all attributes are independent given the value of the class variable; that is,\n",
    "$$p(E|c) = p(x_1, x_2, … , x_n|c) = \\prod\\limits_{i = 1}^n p(x_i|c)$$\n",
    "the resulting classifier is then:\n",
    "$$f_{nb}(E) = {\\frac{p(C = +)}{p(C = -)}} \\prod\\limits_{i = 1}^n {\\frac{p(x_i|c = +)}{p(x_i|c = -)}} $$\n",
    "The function $f_{nb}(E)$ is called a naive Bayesian classifier, or simply naive Bayes (NB). Figure 1 shows an example of naive Bayes. In naive Bayes, each attribute node has no parent except the class node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src='http://i.piccy.info/i9/e3f799752a613ded34b7d17feaf2eca4/1492605985/18456/1138938/NB5.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Naive Bayes is the simplest form of Bayesian network, in which all attributes are independent given the value of the class variable. This is called conditional independence. It is obvious that the conditional independence assumption is rarely true in most real-world applications. A straightforward approach to overcome the limitation of naive Bayes is to extend its structure to represent explicitly the dependencies among attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data for train  \n",
    "train = pd.read_csv('../data/movie_reviews.csv', sep = ',')\n",
    "train_data = train.text\n",
    "train_labels = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data for test \n",
    "test = pd.read_csv('../data/test.csv', sep = ',')\n",
    "test_data = test.text\n",
    "test_labels = test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create validation dataset\n",
    "train_data, train_validation_data, train_labels, train_validation_labels = train_test_split(train.text, train.label, test_size=0.2, random_state=42, stratify=train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Take the last 22 words from each review in the train set\n",
    "train_data = train_data.str.split().apply(lambda x:  ' '.join(x for x in x[-22:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# List of stopwords\n",
    "STOPWORDS = ['by','does', 'was', 'were', 'the', 'of', 'end', 'and', 'is']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert text to vector\n",
    "cvect = CountVectorizer()\n",
    "counts = cvect.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train NB\n",
    "classifier = MultinomialNB(alpha=0.4)\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(binary=True,ngram_range=(1,3),stop_words=STOPWORDS)), ('classifier', classifier)])\n",
    "model = pipeline.fit(X=train_data, y=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.818589869602\n",
      "F1-score : 0.848388598341\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "pred_test = model.predict(train_validation_data)\n",
    "\n",
    "print (\"Accuracy :\", metrics.accuracy_score(train_validation_labels, pred_test))\n",
    "print (\"F1-score :\", metrics.f1_score(train_validation_labels, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8095684803\n",
      "F1-score : 0.825180847399\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "pred_test = model.predict(test_data)\n",
    "\n",
    "print (\"Accuracy :\", metrics.accuracy_score(test_labels, pred_test))\n",
    "print (\"F1-score :\", metrics.f1_score(test_labels, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Linear Models (SVM, Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model overview  \n",
    "\n",
    "\n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm.\n",
    "\n",
    "SVMs are more commonly used in classification problems.\n",
    "\n",
    "SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes.\n",
    "\n",
    "<img src=\"../pictures/svm_intro.png\" alt=\"logistic\" style=\"width: 100%;\"/>\n",
    "\n",
    "\n",
    "** Support Vectors **\n",
    "\n",
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set.\n",
    "What is a hyperplane?\n",
    "\n",
    " \n",
    "As a simple example, for a classification task with only two features (like the image above), you can think of a hyperplane as a line that linearly separates and classifies a set of data.\n",
    "\n",
    "Intuitively, the further from the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it.\n",
    "\n",
    "So when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it.\n",
    "\n",
    "\n",
    "** How do we find the right hyperplane? **\n",
    "\n",
    "\n",
    "Or, in other words, how do we best segregate the two classes within the data?\n",
    "\n",
    "The distance between the hyperplane and the nearest data point from either set is known as the margin. The goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set, giving a greater chance of new data being classified correctly.\n",
    "\n",
    "<img src=\"../pictures/svm_margins.png\" alt=\"logistic\" style=\"width: 70%;\"/>\n",
    "\n",
    "\n",
    "** But what happens when there is no clear hyperplane? **  \n",
    "\n",
    "This is where it can get tricky. Data is rarely ever as clean as our simple example above.  \n",
    "A dataset will often look more like the jumbled balls below which represent a linearly non separable dataset.  \n",
    "In order to classify a dataset like the one above it’s necessary to move away from a 2d view of the data to a 3d view.  \n",
    "\n",
    "Imagine that our two sets of colored balls above are sitting on a sheet and this sheet is lifted suddenly, launching the balls into the air. While the balls are up in the air, you use the sheet to separate them. This ‘lifting’ of the balls represents the mapping of data into a higher dimension. This is known as kernelling. \n",
    "\n",
    "<img src=\"../pictures/svm_kerneling.png\" alt=\"logistic\" style=\"width: 70%;\"/>\n",
    "\n",
    "Because we are now in three dimensions, our hyperplane can no longer be a line. It must now be a plane as shown in the example above. The idea is that the data will continue to be mapped into higher and higher dimensions until a hyperplane can be formed to segregate it.\n",
    "\n",
    "\n",
    "** SVM Uses **\n",
    " \n",
    "SVM is used for text classification tasks such as category assignment, detecting spam and sentiment analysis.  \n",
    "It is also commonly used for image recognition challenges, performing particularly well in aspect-based recognition and color-based classification.  \n",
    "SVM also plays a vital role in many areas of handwritten digit recognition, such as postal automation services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Main params\n",
    "\n",
    "** sklearn.svm.LinearSVC **\n",
    "\n",
    "** C ** : float, default: 1.0  \n",
    "    Inverse of regularization strength; must be a positive float.  \n",
    "    Like in support vector machines, smaller values specify stronger regularization. \n",
    "\n",
    "C: Penalty parameter C of the error term. It also controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
    "    \n",
    "** loss ** : string, ‘hinge’ or ‘squared_hinge’ (default=’squared_hinge’)  \n",
    "    Specifies the loss function. ‘hinge’ is the standard SVM loss (used e.g. by the SVC class) while ‘squared_hinge’ is the square of the hinge loss.\n",
    "\n",
    "** penalty **: string, ‘l1’ or ‘l2’ (default=’l2’)  \n",
    "    Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef_ vectors that are sparse.\n",
    "\n",
    "** tol **: float, optional (default=1e-4)\n",
    "    Tolerance for stopping criteria.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
